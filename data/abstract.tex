\begin{cabstract}
近年来以神经网络为基础的人工智能技术在学术界和工业界得到了广泛应用和发展。随着神经网络模型和训练所需的数据量不断增加，使得单机训练神经网络越来越困难。分布式训练神经网络不仅可以极大减少训练时间，也可对某些单机情况下无法训练的神经网络进行训练。在可预见的未来，分布式训练神经网络将成为必然选择。如何提高分布式训练神经网络的效率和可扩展性则显得尤为重要。

本文针对这一问题提出低精度分布式更新算法LPDU，将原始浮点数梯度转换为低精度数据进行传输，减少同步时间开销，进而提升分布式训练效率，通过混合精度更新算法，保证训练精度。本文通过分析LPDU算法各部分的时间开销和神经网络的参数规模得出LPDU算法在特定神经网络训练中的理论性能提升，并通过实验加以验证。通过对比LPDU算法与原始算法在图像分类，目标检测任务的相关实验,证明了LPDU算法在图像分类与目标检测任务中均能达到与原始更新算法相同的理想精度。同时对分布式训练性能有所提升。在8节点情况下，resnet50的训练效率由原始的84.05\%提升到了87.50\%，VGG网络的训练效率相对于原始的79.42\%提升至了86.55\%，SSD网络相对于原始效率有4.83\%的提升。

基于LPDU算法减少梯度尾数位的思路，以及分类网络和物体检测网络对梯度数据精度要求不同的特点，本文针对分类网和物体检测网络提出了三种极限精度梯度压缩方法EPGC：适用于分类网络的9比特梯度压缩方法和8比特梯度压缩方法，适用于物体检测网络的11比特梯度压缩方法。9比特梯度压缩方法是在浮点数基础上去除所有尾数位，仅使用1个符号位和8个指数位表示梯度；8比特梯度压缩方法是在半精度浮点数基础上去除8位尾数位，使用1个符号位，5个指数位和2个尾数位表示梯度；11比特梯度压缩方法是在半精度浮点数基础上去除5位尾数位，使用1个符号位，5个指数位和5个尾数位表示梯度。为快速验证本文提出的梯度压缩方法的可行性，本文通过在原始浮点数或半精度浮点数基础上对特定尾数位置零的方式模拟这两种压缩方法。通过实验证明这三种梯度压缩算法均能保证神经网络在对应图像分类或物体检测任务中的训练精度，说明通过这三种梯度压缩算法提升分布式训练效率上可行的。

\end{cabstract}
\ckeywords{神经网络; 分布式训练; 低精度; 更新算法; 梯度压缩}

\begin{eabstract}
In recent years, the artificial intelligence technology based on neural networks has been widely used and developed in academia and industry. As neural network models and the amount of data required for training continue to increase, it becomes increasingly difficult to train neural networks in a single machine. The distributed training for neural networks not only greatly shorten the training time, but also provides a solution for some neural networks that cannot be trained in single machine. In the foreseeable future, the distributed training for neural networks will become an inevitable choice. How to improve the efficiency and scalability of distributed training for neural networks is particularly important.

This paper proposes a low-precision distributed update (LPDU) algorithm, which converts the original floating-point gradient into low-precision data format for transmission, which reduces the synchronization time overhead and improves the efficiency of distributed training. The Mixed precision update  (MPU) algorithm can ensure the training accuracy. In this paper, by analyzing the time overhead in each part of LPDU algorithm and the parameter size of the neural network, the theoretical performance improvement of BF16 distributed update algorithm in specific neural network training is obtained and verified by experiments. By comparing the training accuracy curves between the LPDU algorithm and the original algorithm, it proves that the LPDU algorithm can achieve the same ideal precision as the original algorithm in both image classification and object detection tasks. And the efficiency of distributed training has improved. In the case of 8 nodes training, the efficiency of resnet50 has increased from the original 84.05 \% to 87.50\%. The training efficiency of the VGG has increased to 86.55\% compared with the original 79.42\%, and the SSD network has increased by 4.83\% compared with the original efficiency.

Based on the idea of LPDU algorithm that reduces the mantissa of gradient, and the different requirements for the precision of gradient between classification networks and object detection networks. three extreme precision gradient compression (EPGC) algorithms are proposed: 9-bits gradient compression algorithm and 8-bits gradient compression algorithm for classification networks training, 11-bits gradient compression algorithm for object detection networks training.

The 9-bits gradient compression algorithm removes all mantissa bits in the float data format, only using 1 sign bit and 8 exponent bits to represent the gradient. the 8-bits gradient compression algorithm removes 8 bits in the mantissa of half-precision data. using 1 sign bit, 5 exponent bits and 2 mantissa bits represents the gradient. And the 11-bits gradient compression algorithm removes 5 bits in the mantissa of half-precision data. using 1 sign bit, 5 exponent bits and 5 mantissa bits represents the gradient.
 In order to quickly verify the feasibility of the three proposed gradient compression algorithms, this paper simulates the three compression algorithms by zeroing the specific mantissa position based on the original floating point number or half-precision floating point number. Experiments show that the three gradient compression algorithms can guarantee the training accuracy of neural networks in classification or object detection tasks. It is feasible to improve the efficiency of distributed training through these three gradient compression algorithms.

\end{eabstract}
\ekeywords{neural network; distributed training; low precision; update algorithm; gradient compression}


\begin{cabstract}
近年来以神经网络为基础的人工智能技术在学术界和工业界得到了广泛应用和发展。随着神经网络模型和训练所需的数据量不断增加，使得单机训练神经网络越来越困难。分布式训练神经网络不仅可以极大减少训练时间，也可完成某些单机情况下无法训练的神经网络。在可预见的未来，分布式训练神经网络将成为必然选择。如何提高分布式训练神经网络的效率则显得尤为重要。

本文针对这一问题，在提出低精度分布式更新LPDU算法，将原始浮点数梯度转换为BF16格式进行传输，减少同步时间开销，进而提升分布式训练效率，通过混合精度更新算法，保证训练精度。本文通过分析LPDU算法各部分的时间开销和神经网络的参数规模得出LPDU算法在特定神经网络训练中的理论性能提升，并通过实验加以验证。通过对比LPDU算法与原始算法在图像分类，目标检测任务的相关实验证明：LPDU算法在图像分类与目标检测任务中均能达到与原始更新算法相同的理想精度。同时对分布式训练性能有所提升。在8节点情况下，resnet50的效率由原始的84.05\%提升到了87.50\%，VGG网络的训练效率相对于原始的79.42\%提升至了86.55\%,SSD网络相对于原始效率有4.83\%的提升。

基于LPDU算法减少梯度尾数位的思路，本文提出两种极限精度梯度压缩EPGC方法：9比特梯度压缩方法和8比特梯度压缩方法。9比特梯度压缩方法是在浮点数基础上去除所有尾数位，仅使用1个符号位和8个指数位表示梯度；8比特梯度压缩方法是在半精度浮点数基础上去除8位尾数位，使用1个符号位，5个指数位和2个尾数位表示梯度。为快速验证本文提出的梯度压缩方法的可行性，本文通过在原始浮点数或半精度浮点数基础上对特定尾数位置零的方式模拟这两种压缩方法。通过实验证明这两种梯度压缩算法均能保证神经网络在图像分类任务中的训练精度，说明通过这两种梯度压缩算法提升分布式训练效率上可行的。

\end{cabstract}
\ckeywords{神经网络; 分布式训练; 低精度；更新算法; 梯度压缩}

\begin{eabstract}
In recent years, artificial intelligence technology based on neural networks has been widely used and developed in academia and industry. As neural network models and the amount of data required for training continue to increase, it becomes increasingly difficult to train neural networks in a single machine. The distributed training neural network not only can greatly shorten the training time, but also can train some neural networks that cannot be trained in single machine. In the foreseeable future, distributed training neural networks will become an inevitable choice. How to improve the efficiency of distributed training neural networks is particularly important.

This paper proposes a low-precision distributed update (LPDU) algorithm, which converts the original floating-point gradient into BF16 format for transmission, which reduces the synchronization time overhead and improves the distributed training efficiency. The Mixed precision update  (MPU) algorithm can ensure the training accuracy. In this paper, by analyzing the time overhead in each part of LPDU algorithm and the parameter size of neural network, the theoretical performance improvement of BF16 distributed update algorithm in specific neural network training is obtained and verified by experiments. By comparing the training accuracy curves between the LPDU algorithm and the original algorithm, it proves that the LPDU algorithm can achieve the same ideal precision as the original update algorithm in both image classification and object detection tasks. And the performance of distributed training has improved. In the case of 8 nodes training, the efficiency of resnet50 has increased from the original 84.05 \% to 87.50\%. The training efficiency of the VGG has increased to 86.55\% compared with the original 79.42\%, and the SSD network has increased by 4.83\% compared with the original efficiency.

Based on the idea of LPDU algorithm to reduce the mantissa of gradient, two extreme precision gradient compression (EPGC) algorithms are proposed: 9-bits gradient compression algorithm and 8-bits gradient compression algorithm. The 9-bits gradient compression algorithm removes all mantissa bits in the float data format, using only 1 sign bit and 8 exponent bits to represent the gradient; the 8-bits gradient compression algorithm removes 8 bits in the mantissa of half-precision data. using one sign bit, five exponent bits and two mantissa bits represent the gradient. In order to quickly verify the feasibility of the two proposed gradient compression algorithms proposed, this paper simulates the two compression algorithms by zeroing the specific mantissa position based on the original floating point number or half-precision floating point number. Experiments show that both gradient compression algorithms can guarantee the training accuracy of neural networks in image classification tasks. It is feasible to improve the efficiency of distributed training through these two gradient compression algorithms.

\end{eabstract}
\ekeywords{neural network; distributed training; low precision; update algorithm; gradient compression}


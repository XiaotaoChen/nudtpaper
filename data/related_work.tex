\chapter{相关研究}
\section{分布式训练神经网络的特点}
1.精度，2.性能
正文内容
1. 神经网络的发展，分布式深度学习的发展（horovod，ps），半精度的发展；数据并行方法
\section{分布式深度学习系统的发展}
分布式论文 ng 2012年识别猫开始
深度学习重新成为一个热题源于2012年底NIPS上面那篇AlexNet[18]的论文，用卷积神经 网络(CNN)跑ImageNet，并且刷新了很多记录，从此开启了深度学习的新篇章。
同年，对应的分布式深度学习系统也应运而生。Jeffrey Dean等提出的DistBelief[19]， 即Google的第一代深度学习系统，其核心实现是把分布式机器学习里面的数据并行(data parallelism)和模型并行(model parallelism)用在深度学习上，通过这两种并行方法 把深度学习大规模化到集群。同时，论文提出了分布式异步SGD—DownPour SGD。
随后，考虑到分布式训练网络过程中的同步通信开销，Qirong Ho[20]等提出了松弛一致 性模型:Stale Synchronous Parallel(SSP)。解决了严格同步(BSP)模型下木桶效应导 致同步时间过长的问题，提高了计算效率。并基于该模型，搭建了Petuum系统。同时，Adam Coates[21]等第一次搭建了一个GPU集群:COTS HPC system，该系统通过模型并行分布式训 练网络模型。紧接着Microsoft推出了基于参数服务器的深度学习系统Adam[22]。
分布式深度学习中一个关键的通信组件是参数服务器(Parameter Server)，最早由Alex Smola提出，因其简单、高效的特点，被广泛用于分布式深度学习进行参数同步。最著名的 参数服务器框架要数Mu Li等人做的Parameter Server[23]，并进一步开发出MxNet。
分布式深度学习中最经典，最基本的问题在于如果减少同步参数的等待时间。基于不 同的考虑，分别有BSP，SSP，ASP三种同步策略。其中，ASP模型使用完全异步的同步方式， 同步开销最小，但因为存在过期参数的问题，使得单次训练的效率低于BSP模型，且最终结 果不一定收敛，相关工作有[24-27]等。SSP模型，是BSP与ASP的一个折中，松弛了严格同步条 件，减少了同步开销时间，相关工作有[28,29]等。BSP模型因为其存在木桶效应，随着集群的 增大，同步开销时间无法保障，但单次迭代训练的效率最高，能够保证收敛。目前前沿的 分布式深度学习工作都是基于BSP模型。如Priya Goyal[9]等用ResNet-50[8]在1小时内训好 ImageNet，Yang You[10]等用24分钟训完ImageNet以及Samuel L. Smith[11]等进一步将 mini-batch size扩大到65536等
正文内容
\section{混合精度训练发展，目前现有硬件不支持的问题}
正文
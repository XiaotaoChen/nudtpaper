\chapter{总结与展望}
\section{工作总结}
优化分布式训练神经网络的性能对神经网络的发展有至关重要的作用。随着神经网络的发展，数据量和神经网络规模呈爆发性增长趋势，在可预见的将来单机训练将很难满足要求，分布式训练神经网络是必然选择，优化分布式训练神经网络的性能显得尤为重要。针对这一问题，本文提出低精度分布式更新LPDU算法，通过使用低精度数据传输梯度的方法，减少同步通信过程中的数据量，进而减少同步开销，提升分布式训练效率。在此基础上，本文提出三种极限精度梯度压缩EPGC方法：适用于图像分类网络的9比特梯度压缩方法和8比特梯度压缩方法，适用于物体检测网络的11比特梯度压缩方法，这些方法均能保证对应网络不损失训练精度的前提下，进一步减少同步时间开销，提高分布式系统的可扩展性。本文主要研究工作和研究成果如下：

1. 低精度分布式更新算法。该算法旨在保证神经网络训练精度的情况下，提升分布式训练神经网络的性能。通过使用低精度数据传输梯度，达到减少同步数据量，减少同步时间开销的目的。为避免低精度梯度数据造成的精度损失对神经网络训练精度产生影响，本文提出混合精度更新算法对网络进行更新，避免在更新过程中造成精度的进一步损失。实验证明本文提出的低精度分布式更新LPDU算法可在不影响神经网络训练精度的情况下，提升分布式训练神经网络的效率。

2. 极限精度梯度压缩方法：适用于图像分类任务的9比特梯度压缩方法与8比特梯度压缩方法，适用于物体检测任务的11比特梯度压缩方法。在LPDU算法基础上，为提升分布式系统的可扩展性和训练效率，我们进一步压缩梯度数据的比特位。针对分类任务对梯度数据精度要求偏低的特点，本文提出基于原始浮点数去除所有尾数位的9比特梯度压缩方法，仅保留1个符号位与8个指数位。为快速验证本文所提出的梯度压缩方法对神经网络训练精度的影响，本文通过对浮点数梯度特定尾数位清零的方法模拟相应的梯度压缩方法。根据近年来业界将半精度浮点数应用于神经网络训练中的相关工作，本文还提出适用于图像分类任务的基于半精度浮点数的8比特梯度压缩方法，保留1个符号位，5个指数位和2个尾数位。针对物体检测网络对梯度数据精度要求偏高的特点，本文提出了11比特梯度压缩方法。

在LPDU算法相关实验中，本文首先通过分析LPDU算法中涉及到的各部分计算的时间开销，以及特定神经网络的参数量大小与分布，从理论上分析LPDU算法在特定神经网络训练中对分布式训练效率的性能提升。真实训练过程中测得的性能提升与理论分析相一致，说明算法实现与理论分析的正确性。而后通过比较LPDU算法与原始更新算法在分类网、物体检测网的收敛精度与训练速度，说明LPDU算法在不影响神经网络收敛精度情况下，对分布式训练神经网络性能有一定提升。

在极限梯度压缩方法相关实验中，本文通过对原始浮点数或半精度浮点数特定尾数位清零的方法模拟对应梯度压缩方法。通过9比特，8比特梯度压缩方法在分类网络中的收敛精度与原始收敛精度的对比，说明本文提出的9比特，8比特梯度压缩方法在分类网中的有效性。通过11比特梯度压缩方法在物体检测网络中的收敛精度与原始收敛精度的对比，说明将本文提出的11比特梯度压缩方法应用于物体检测网络的可行性。 
\section{工作展望}
本文提出的三种极限梯度压缩EPGC方法均通过在浮点数或半精度浮点数上模拟实现。现阶段只证明对应梯度压缩方法在分布式训练分类网或物体检测网络的可行性。对分布式系统可扩展性和训练效率的提升效果有待进一步实现对应梯度压缩算法后在真实训练场景中评测。下一步我们将基于horovod实现这三种梯度压缩算法，达到提升分布式系统的可扩展性和训练效率的目的。

对比9比特梯度压缩方法与8比特梯度压缩方法可知，可在8比特梯度压缩基础上，继续减少尾数位，极限情况下仅使用6比特位表示梯度。通过验证该压缩方法在分类网上的收敛精度，判断该梯度压缩方法用于分布式训练分类网的可行性。另一方面，针对分类任务与物体检测任务对梯度数据精度要求不同的特点，本文希望将传统压缩算法引入到分布式训练神经网络训练的通信过程中，根据神经网络对梯度数据精度的不同要求，对传统压缩算法进行优化改进，使得压缩算法在对应任务的神经网络中达到更优的效果。


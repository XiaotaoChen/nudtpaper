\chapter{总结与展望}
\section{工作总结}
优化分布式训练神经网络的性能对神经网络的发展有至关重要的作用，是所有神经网络算法的基础。随着神经网络的发展，数据量和神经网络规模呈现爆发性增长趋势，在可预见的将来单机训练已经很难满足要求，分布式训练神经网络将是必然选择，优化分布式训练神经网络的性能显得尤为重要。针对这一问题，本文提出BF16分布式更新算法，通过减少原始浮点梯度的尾数位，以减少同步梯度过程中的数据量，进而减少同步开销，提升分布式训练效率。在此基础上，本文提出9比特梯度压缩方法和8比特梯度压缩方法，以进一步减少同步时间开销。本文的主要研究工作和研究成果如下：

1. BF16分布式更新算法。该算法旨在保证神经网络训练精度的情况下，提升分布式训练神经网络的性能。通过使用BF16格式数据传输梯度，减少了梯度数据的尾数位，达到减少同步数据量，减少同步时间开销的目的。本算法受启发于google公司使用BF16数据格式训练神经网络的相关研究。其研究表明在神经网络训练过程中，完全使用BF16数据格式训练神经网络能达到浮点数训练相同的精度，并根据这一研究成果开发出来首款支持BF16计算硬件：TPU。故本文提出将原始浮点数梯度转换为BF16数据格式，以减少分布式同步过程中的通信量。

2. 9比特梯度压缩方法与8比特梯度压缩方法。在BF16更新算法基础上，为进一步压缩梯度数据比特位，本文提出基于原始浮点数去除所有尾数位的9比特压缩方法，仅保留1个符号位与8个指数位。为快速验证本文所提出的压缩方法对神经网络训练精度的影响，本文通过对浮点数梯度特定尾数位清零的方法模拟相应的压缩方法。根据近年来业界将半精度浮点数应用于神经网络训练中的相关工作，本文还提出基于半精度浮点数的8比特梯度压缩方法，保留1个符号位，5个指数位和2个尾数位。

在BF16更新算法相关实验中，本文首先通过分析BF16更新算法中涉及到的各部分操作的时间开销，以及特定神经网络的参数量大小与分布，从理论上分析BF16更新算法在特定神经网络训练中对分布式训练效率的性能提升。真实训练过程中测得的性能提升与理论分析相一致，说明算法实现与理论分析的正确性。而后通过比较BF16更新算法与原始更新算法在分类网，物体检测网的收敛效果与训练速度,说明BF16更新算法在不影响神经网络精度情况下，对分布式训练神经网络性能有一定提升。

在9比特梯度压缩方法与8比特梯度压缩方法相关实验中，本文通过对原始浮点数或半精度浮点数特定尾数位清零的方法模拟对应压缩方法。通过这两种压缩方法下神经网络的收敛精度与原始收敛精度的对比，说明本文提出的两种压缩方法的有效性。
\section{工作展望}
本文提出的9比特梯度压缩方法与8比特梯度压缩方法均通过在浮点数或半精度浮点数上模拟实现。现阶段只证明该压缩方法在分布式训练分类网中的可行性。对分布式训练效率的提升效果有待进一步实现这两种压缩算法后，在真实训练场景中的表现。下一步我们将基于horovod实现这两种压缩算法，进一步提升分布式训练效率。

为验证9比特梯度压缩算法与8比特梯度压缩算法并不仅仅适用于分类任务，我们将进一步在物体检测，自然语言处理任务中使用这两种压缩算法。根据不同任务中压缩算法对网络性能的影响，改进这两种压缩算法，找到一种普遍适用于神经网络的梯度压缩算法。
\chapter{总结与展望}
\section{工作总结}
优化分布式训练神经网络的性能对神经网络的发展有至关重要的作用。随着神经网络的发展，数据量和神经网络规模呈爆发性增长趋势，在可预见的将来单机训练已经很难满足要求，分布式训练神经网络将是必然选择，优化分布式训练神经网络的性能显得尤为重要。针对这一问题，本文提出低精度分布式更新LPDU算法，通过使用低精度数据传输梯度的方法，减少同步通信过程中的数据量，进而减少同步开销，提升分布式训练效率。在此基础上，本文提出两种极限精度梯度压缩EPGC方法：9比特梯度压缩方法和8比特梯度压缩方法，以进一步减少同步时间开销，提高分布式系统的可扩展性。本文主要研究工作和研究成果如下：

1. 低精度分布式更新算法。该算法旨在保证神经网络训练精度的情况下，提升分布式训练神经网络的性能。通过使用低精度数据传输梯度，减少了梯度数据的尾数位，达到减少同步数据量，减少同步时间开销的目的。本算法受启发于google公司使用BF16数据格式训练神经网络的相关研究。其研究表明在神经网络训练过程中，完全使用BF16数据格式训练神经网络能达到浮点数训练相同的精度，并根据这一研究成果开发出来首款支持BF16计算硬件：TPU。故本文提出将原始浮点数梯度转换为BF16数据格式，以减少分布式同步过程中的通信量。

2. 极限精度梯度压缩方法：9比特梯度压缩方法与8比特梯度压缩方法。在LPDU算法基础上，为提升分布式系统的可扩展性和训练效率，我们进一步压缩梯度数据的比特位，本文提出基于原始浮点数去除所有尾数位的9比特梯度压缩方法，仅保留1个符号位与8个指数位。为快速验证本文所提出的梯度压缩方法对神经网络训练精度的影响，本文通过对浮点数梯度特定尾数位清零的方法模拟相应的梯度压缩方法。根据近年来业界将半精度浮点数应用于神经网络训练中的相关工作，本文还提出基于半精度浮点数的8比特梯度压缩方法，保留1个符号位，5个指数位和2个尾数位。

在LPDU算法相关实验中，本文首先通过分析LPDU算法中涉及到的各部分操作的时间开销，以及特定神经网络的参数量大小与分布，从理论上分析LPDU算法在特定神经网络训练中对分布式训练效率的性能提升。真实训练过程中测得的性能提升与理论分析相一致，说明算法实现与理论分析的正确性。而后通过比较LPDU算法与原始更新算法在分类网，物体检测网的收敛精度与训练速度,说明LPDU算法在不影响神经网络收敛精度情况下，对分布式训练神经网络性能有一定提升。

在9比特梯度压缩方法与8比特梯度压缩方法相关实验中，本文通过对原始浮点数或半精度浮点数特定尾数位清零的方法模拟对应梯度压缩方法。通过这两种梯度压缩方法在分类网和物体检测网络中的收敛精度与原始收敛精度的对比，说明本文提出的两种梯度压缩方法在分类网中的有效性，以及不适用于物体检测网络的原因，并提出了下一步需要做的工作。
\section{工作展望}
本文提出的9比特梯度压缩方法与8比特梯度压缩方法均通过在浮点数或半精度浮点数上模拟实现。现阶段只证明该梯度压缩方法在分布式训练分类网中的可行性。对分布式系统可扩展性和训练效率的提升效果有待进一步实现这两种梯度压缩算法后，在真实训练场景中的表现。下一步我们将基于horovod实现这两种梯度压缩算法，达到提升分布式系统的可扩展性和训练效率。

对比9比特梯度压缩方法与8比特梯度压缩方法可知，在8比特梯度压缩基础上，继续减少尾数位，极限情况下仅使用6比特位表示梯度，并验证该压缩方法在分类网上的收敛精度，进而判断该梯度压缩方法用于分布式训练分类网的可行性。另一方面，针对8比特梯度压缩方法不能保证物体检测网络收敛精度的问题，下一步我们将基于8比特梯度压缩方法逐渐增加尾数比特位，直至其在物体检测网络中的收敛精度达到原始浮点数训练精度为止。
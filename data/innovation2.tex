\chapter{神经网络中梯度压缩方法探索}
受上一章节BF16更新算法启发，本章希望在不影响神经网络训练精度的情况下，尽可能减少梯度所需的比特位数，为进一步减少分布式通信开销提供数据压缩依据。本章将主要介绍梯度压缩的实现方法，以及对应压缩方式在resnet50中的训练精度。通过resnet50最终的收敛精度判断相应压缩方法的有效性。
\section{引言}
根据第二部分可知，分布式训练效率的高低主要取决于同步开销的大小，其主要取决于同步数据量的大小。在不影响训练精度的情况下，将梯度尽可能地压缩则可减少同步开销，提高分布式训练效率。本章将介绍本节对梯度压缩探索的具体实现方法以及各个压缩方式下resnet50的最终收敛精度。
\section{基于浮点数9比特位压缩方法}
由第三章BF16分布式更新算法实现可知，基于horovod要实现新的数据压缩方式，则需在MPI中新增一个数据类型来解析该压缩数据，并实现一个该数据类型的求和算法供MPI调用。为快速验证本文提出的梯度压缩方法对神经网络训练精度的影响，本节压缩方法均通过对原始FP32浮点数的对应比特位置零来模拟对应的压缩方法。该模拟方式可避免对MPI的改动，使得本节提出的压缩方法能够快速得到验证。下面将对本节探索出的两种压缩方式进行详细介绍。

根据上一章BF16更新算法可知，BF16数据格式相对于原始FP32数据而言，去除了浮点数尾数中的后16比特位，仅保留7位有效比特位。本压缩算法根据这一思路，在BF16数据基础上，进一步减少尾数有效位，在保证训练精度情况下，尽可能减少尾数比特位，以减少分布式同步中的通信量。

\subsection{具体实现与实验结果}
\begin{figure}[htp]
\centering
\includegraphics[width=10cm]{simulate_11bits}
\caption{模拟11比特数据表示示意图}
\label{fig:simulate_11bits}
\end{figure}
本节通过对原始FP32浮点数尾数位置零的方法模拟这种压缩方式。如在FP32浮点数基础上仅保留两位有效尾数位，形成11比特的数据格式，可通过将FP32浮点数的低21位清零达到相同效果，如图~\ref{fig:simulate_11bits}所示。

可知低21比特位清零后的浮点数等效于11比特位数据格式所表示数据。经实验验证：在同步梯度数据时仅保留两比特位的尾数情况下，神经网络也可收敛到原始浮点数梯度相同精度。在11比特有效位基础上，继续减少尾数位，保留1位尾数、去除所有尾数位情况下，在分类网上均能达到理想收敛精度。在去除所有尾数位仅使用9比特数据表示梯度情况下，resnet50的收敛曲线如图~\ref{fig:simulate_9bits_acc}所示。

\begin{figure}[htp]
\centering
\includegraphics[width=10cm]{simulate_9bits_acc}
\caption{9比特位梯度压缩方法与原始浮点数表示训练精度曲线}
\label{fig:simulate_9bits_acc}
\end{figure}

保留2位尾数，1位尾数，不保留尾数情况下，resnet50在验证集精度如表~\ref{tab:simulate_11_10_9bits_acc}所示。所有结果均在4节点8实例配置下训练得到，由表~\ref{tab:simulate_11_10_9bits_acc}可知，在原始浮点梯度基础上，仅保留2位尾数，1位尾数，不保留尾数情况下，在分类网中均能达到与浮点数梯度相同的精度，说明在分类网中仅使用9比特数据（1个符号位，8个指数位）表示梯度即可达到原始训练精度，通过去除浮点数中所有尾数位的方法可在不影响神经网络训练精度前提下，减少分布式同步开销提升分布式训练性能。

\begin{table}[htb]
\centering
\noindent\begin{minipage}{0.45\textwidth}
\centering
\caption{不同尾数位下resnet50精度}
\label{tab:simulate_11_10_9bits_acc}
\begin{tabular}{p{2cm}p{2cm}}
\toprule[1.5pt]
有效尾数位 & 精度(\%) \\\midrule[1pt]
2 & 76.08 \\
1 & 76.01 \\
0 & 75.87 \\
\midrule[1pt]
\end{tabular}
\end{minipage}
\end{table}
本节提出的9比特位梯度数据压缩方法还未在目标检测网中进行试验，下一步将在目标检测网络中使用该压缩方法进行分布式训练，以验证该9比特位梯度数据压缩方法是否同样适用于目标检测网络。

\section{基于半精度浮点数的8比特数据压缩方法}
由本文第二章可知，半精度浮点数也可用于训练神经网络，也能保证网络的训练精度与原始浮点数训练一致。半精度浮点数与BF16数据格式相比，少了3位指数位，仅5位指数位。根据上一节9比特位梯度压缩方法可知，在仅保留2位，1位尾数或不保留尾数位情况下，均不会影响神经网络训练精度。本节结合半精度浮点数特点，去除16位半精度浮点数中低8位尾数，仅保留两个有效尾数位，使用8比特数据表示梯度，以此将梯度压缩成单字节数据。
\subsection{具体实现与实验分析}
为快速验证本节所提出的梯度压缩方法的有效性，本节通过将半精度浮点数特定尾数位清零的方式模拟该压缩方法。原理如图~\ref{fig:simulate_8bits}所示。在4节点8实例情况下，使用该方法将梯度数据压缩成单字节数据，resnet50的收敛曲线如图~\ref{fig:simulate_8bits_acc}所示。可知本节提出的8比特数据压缩方法同样能达到原始训练精度:76.1\%，说明本节提出的压缩方法可在不影响神经网络精度情况下，减小分布式训练神经网络中的通信量，从而提升分布式训练效率。

\begin{figure}[htp]
\centering
\includegraphics[width=10cm]{simulate_8bits}
\caption{模拟8比特数据表示示意图}
\label{fig:simulate_8bits}
\end{figure}

\begin{figure}[htp]
\centering
\includegraphics[width=10cm]{simulate_8bits_acc}
\caption{8比特位梯度压缩方法与原始浮点数表示训练精度曲线}
\label{fig:simulate_8bits_acc}
\end{figure}

同时，根据上一节提出的9比特压缩方法可知，可进一步在半精度浮点数上减少尾数位或去除所有尾数位，极限情况下仅使用6比特数据表示梯度。下一步将对该种可能的压缩方法进行尝试验证。也将在目标检测网络训练中应用本节提出的方法以说明该8比特压缩算法同样适用于目标检测网络。

\section{本章小结}
本章基于减少分布式训练神经网络通信开销的目的，希望通过减少梯度表示比特位来减少通信数据量，提出9比特梯度压缩方法和8比特梯度压缩方法。经验证本章提出的两种压缩方法在resnet50网络中均能达到原始训练精度，说明这两种压缩方法的可行性。同时也指出下一步工作：将本章提出的两种压缩算法应用于目标检测网络，以说明这两种压缩算法对神经网络的普遍适用性。基于半精度浮点数提出的8比特位压缩方法还有进一步压缩的空间，值得进一步尝试验证。









%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:

\begin{last_words}
  本文针对如何优化提升分布式深度学习系统的性能问题，根据神经网络对低精度数据的友好性，提出低精度分布式更新算法，在保证神经网络训练精度的前提下，提升了分布式训练神经网络的训练效率。根据神经网络中梯度数据范围普遍较小，图像分类任务和物体检测任务对数据精度要求不同的特点，分别提出适用于分类网络的9比特，8比特梯度压缩算法，适用于物体检测网络的11比特梯度压缩算法。可极大减少数据通信量，进而提升分布式系统的训练效率和可扩展性。

  本文首先对课题的研究背景和意义进行了简单介绍，指出随着神经网络的蓬勃发展，神经网络模型趋向于复杂化，计算量呈指数型增长，且用于训练神经网络的数据量越来越庞大，分布式训练神经网络是必然选择。紧接着对分布式深度学习系统中涉及到的神经网络模型，通信框架，优化算法以及低精度训练神经网络的相关研究进展。然后对本文两部分主要工作：低精度分布式更新算法和极限精度梯度压缩算法。最后对课题进行了总结并指出了下一步工作。

  低精度分布式更新算法：旨在保证神经网络训练精度的前提下，提升分布式训练神经网络的性能。算法主要包含两部分：低精度数据通信算法和混合精度更新算法。通过低精度数据通信算法，使用低精度数据进行通信，使得数据通信量减半，可减小通信时间，提高系统训练效率；通过混合精度更新算法使用低精度梯度数据对神经网络进行更新，保证了神经网络训练精度。

  极限精度梯度压缩算法：考虑到梯度数据普遍较少，绝大部分梯度数据绝对值在0～1之间，且图像分类任务和物体检测任务对梯度数据精度要求不同的特点，分别提出适用于分类网络的9比特，8比特梯度压缩方法和适用于检测网络的11比特梯度压缩方法。实验证明本文提出的3种极限精度梯度压缩方法在各自任务中均能保证网络精度，极限精度梯度压缩算法可极大减少分布式训练过程中的数据通信量，减少通信时间，从而提升系统训练效率和可扩展性。

\end{last_words}
